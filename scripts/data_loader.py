#!/usr/bin/env python3
"""
INTERNCRANE æ•°æ®åŠ è½½æ¨¡å—

è‡ªåŠ¨è¯»å–INTERNCRANEè®­ç»ƒæ—¶ç”Ÿæˆçš„æ•°æ®æ‹†åˆ†ï¼Œæ”¯æŒä»å®éªŒç›®å½•åŠ è½½
train/valid/testæ•°æ®ï¼ŒåŒæ—¶ä¿æŒç”¨æˆ·è‡ªå®šä¹‰æ•°æ®çš„çµæ´»æ€§ã€‚
"""

import os
import json
import pandas as pd
import numpy as np
from typing import Dict, List, Any, Union, Optional, Tuple
import warnings

def load_experiment_config(experiment_dir: str) -> Dict[str, Any]:
    """
    ä»å®éªŒç›®å½•åŠ è½½é…ç½®æ–‡ä»¶
    
    Args:
        experiment_dir: å®éªŒç›®å½•è·¯å¾„
    
    Returns:
        dict: å®éªŒé…ç½®
    """
    config_path = os.path.join(experiment_dir, "run_config.json")
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)
    
    return config

def load_original_data_splits(experiment_dir: str) -> Dict[str, pd.DataFrame]:
    """
    åŠ è½½åŸå§‹æ•°æ®æ‹†åˆ†
    
    Args:
        experiment_dir: å®éªŒç›®å½•è·¯å¾„
    
    Returns:
        dict: åŒ…å«train, val, testæ•°æ®çš„å­—å…¸
    """
    splits_dir = os.path.join(experiment_dir, "original_data_splits")
    
    if not os.path.exists(splits_dir):
        raise FileNotFoundError(f"åŸå§‹æ•°æ®æ‹†åˆ†ç›®å½•ä¸å­˜åœ¨: {splits_dir}")
    
    data_splits = {}
    
    # åŠ è½½å„ä¸ªæ•°æ®é›†
    split_files = {
        'train': 'train_original_data.csv',
        'val': 'val_original_data.csv', 
        'test': 'test_original_data.csv'
    }
    
    for split_name, filename in split_files.items():
        filepath = os.path.join(splits_dir, filename)
        if os.path.exists(filepath):
            try:
                df = pd.read_csv(filepath)
                data_splits[split_name] = df
                print(f"âœ“ åŠ è½½ {split_name} æ•°æ®: {len(df)} æ ·æœ¬")
            except Exception as e:
                warnings.warn(f"æ— æ³•åŠ è½½ {split_name} æ•°æ®: {e}")
        else:
            if split_name != 'val':  # valå¯èƒ½ä¸å­˜åœ¨ï¼ˆäº¤å‰éªŒè¯æ¨¡å¼ï¼‰
                warnings.warn(f"{split_name} æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
    
    # åŠ è½½æ‹†åˆ†æ‘˜è¦
    summary_path = os.path.join(splits_dir, "data_split_summary.csv")
    if os.path.exists(summary_path):
        summary_df = pd.read_csv(summary_path)
        print("\nğŸ“Š æ•°æ®æ‹†åˆ†æ‘˜è¦:")
        for _, row in summary_df.iterrows():
            print(f"  {row['split']}: {row['count']} æ ·æœ¬ ({row['percentage']})")
    
    return data_splits

def load_processed_data_splits(experiment_dir: str) -> Dict[str, np.ndarray]:
    """
    åŠ è½½å¤„ç†åçš„ç‰¹å¾æ•°æ®æ‹†åˆ†
    
    Args:
        experiment_dir: å®éªŒç›®å½•è·¯å¾„
    
    Returns:
        dict: åŒ…å«Xå’Œyæ•°æ®çš„å­—å…¸
    """
    data_splits_dir = os.path.join(experiment_dir, "data_splits")
    
    if not os.path.exists(data_splits_dir):
        raise FileNotFoundError(f"æ•°æ®æ‹†åˆ†ç›®å½•ä¸å­˜åœ¨: {data_splits_dir}")
    
    # æŸ¥æ‰¾æ•°æ®é›†å‰ç¼€ï¼ˆé€šå¸¸æ˜¯å®éªŒåç§°ï¼‰
    files = os.listdir(data_splits_dir)
    prefixes = set()
    for file in files:
        if file.endswith('.csv'):
            parts = file.split('_')
            if len(parts) >= 2:
                prefixes.add(parts[0])
    
    if not prefixes:
        raise FileNotFoundError(f"åœ¨ {data_splits_dir} ä¸­æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶")
    
    # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„å‰ç¼€
    prefix = sorted(prefixes)[0]
    print(f"ğŸ” ä½¿ç”¨æ•°æ®é›†å‰ç¼€: {prefix}")
    
    processed_data = {}
    
    # åŠ è½½å„ç§æ•°æ®æ–‡ä»¶
    data_files = {
        'X_train': f'{prefix}_X_train.csv',
        'y_train': f'{prefix}_y_train.csv',
        'X_val': f'{prefix}_X_val.csv',
        'y_val': f'{prefix}_y_val.csv',
        'X_test': f'{prefix}_X_test.csv',
        'y_test': f'{prefix}_y_test.csv'
    }
    
    for data_name, filename in data_files.items():
        filepath = os.path.join(data_splits_dir, filename)
        if os.path.exists(filepath):
            try:
                df = pd.read_csv(filepath)
                # è½¬æ¢ä¸ºnumpyæ•°ç»„
                if data_name.startswith('y_'):
                    processed_data[data_name] = df.iloc[:, 0].values  # ç›®æ ‡å€¼é€šå¸¸æ˜¯ç¬¬ä¸€åˆ—
                else:
                    processed_data[data_name] = df.values
                print(f"âœ“ åŠ è½½ {data_name}: {df.shape}")
            except Exception as e:
                if data_name not in ['X_val', 'y_val']:  # valå¯èƒ½ä¸å­˜åœ¨
                    warnings.warn(f"æ— æ³•åŠ è½½ {data_name}: {e}")
    
    return processed_data

def prepare_stacking_data(experiment_dir: str, 
                         use_original: bool = True) -> Dict[str, Any]:
    """
    ä¸ºæ¨¡å‹å †å å‡†å¤‡æ•°æ®
    
    Args:
        experiment_dir: å®éªŒç›®å½•è·¯å¾„
        use_original: æ˜¯å¦ä½¿ç”¨åŸå§‹æ•°æ®ï¼ˆTrueï¼‰è¿˜æ˜¯å¤„ç†åçš„ç‰¹å¾æ•°æ®ï¼ˆFalseï¼‰
    
    Returns:
        dict: åŒ…å«éªŒè¯å’Œæµ‹è¯•æ•°æ®çš„å­—å…¸
    """
    config = load_experiment_config(experiment_dir)
    
    if use_original:
        # ä½¿ç”¨åŸå§‹æ•°æ®ï¼ˆæ¨èç”¨äºéªŒè¯æ¨¡å‹å †å ï¼‰
        data_splits = load_original_data_splits(experiment_dir)
        
        # æå–ç›®æ ‡åˆ—
        target_col = config['data']['single_file_config']['target_col']
        
        result = {
            'config': config,
            'target_column': target_col,
            'split_mode': config.get('split_mode', 'unknown')
        }
        
        # å‡†å¤‡éªŒè¯æ•°æ®å’Œæ ‡ç­¾
        if 'val' in data_splits and len(data_splits['val']) > 0:
            val_df = data_splits['val']
            result['validation_data'] = val_df.drop(columns=[target_col]).to_dict('records')
            result['validation_labels'] = val_df[target_col].tolist()
            print(f"âœ“ å‡†å¤‡éªŒè¯æ•°æ®: {len(result['validation_data'])} æ ·æœ¬")
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®å’Œæ ‡ç­¾
        if 'test' in data_splits and len(data_splits['test']) > 0:
            test_df = data_splits['test']
            result['test_data'] = test_df.drop(columns=[target_col]).to_dict('records')
            result['test_labels'] = test_df[target_col].tolist()
            print(f"âœ“ å‡†å¤‡æµ‹è¯•æ•°æ®: {len(result['test_data'])} æ ·æœ¬")
        
        # å¦‚æœæ²¡æœ‰éªŒè¯é›†ï¼Œä½¿ç”¨éƒ¨åˆ†è®­ç»ƒæ•°æ®ä½œä¸ºéªŒè¯
        if 'validation_data' not in result and 'train' in data_splits:
            train_df = data_splits['train']
            # ä½¿ç”¨å20%çš„è®­ç»ƒæ•°æ®ä½œä¸ºéªŒè¯
            val_size = max(10, int(len(train_df) * 0.2))
            val_df = train_df.tail(val_size)
            
            result['validation_data'] = val_df.drop(columns=[target_col]).to_dict('records')
            result['validation_labels'] = val_df[target_col].tolist()
            print(f"âš ï¸  ä½¿ç”¨è®­ç»ƒæ•°æ®å°¾éƒ¨ä½œä¸ºéªŒè¯: {len(result['validation_data'])} æ ·æœ¬")
        
        return result
    
    else:
        # ä½¿ç”¨å¤„ç†åçš„ç‰¹å¾æ•°æ®
        processed_data = load_processed_data_splits(experiment_dir)
        
        result = {
            'config': config,
            'split_mode': config.get('split_mode', 'unknown'),
            'processed_data': processed_data
        }
        
        # å‡†å¤‡éªŒè¯æ•°æ®
        if 'X_val' in processed_data and 'y_val' in processed_data:
            result['validation_features'] = processed_data['X_val']
            result['validation_labels'] = processed_data['y_val']
            print(f"âœ“ å‡†å¤‡éªŒè¯ç‰¹å¾: {processed_data['X_val'].shape}")
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        if 'X_test' in processed_data and 'y_test' in processed_data:
            result['test_features'] = processed_data['X_test']
            result['test_labels'] = processed_data['y_test']
            print(f"âœ“ å‡†å¤‡æµ‹è¯•ç‰¹å¾: {processed_data['X_test'].shape}")
        
        return result

def format_sample_for_prediction(sample_dict: Dict[str, Any], 
                                config: Dict[str, Any]) -> Dict[str, Any]:
    """
    å°†åŸå§‹æ•°æ®æ ·æœ¬æ ¼å¼åŒ–ä¸ºé¢„æµ‹æ‰€éœ€çš„æ ¼å¼
    
    Args:
        sample_dict: åŸå§‹æ ·æœ¬å­—å…¸
        config: å®éªŒé…ç½®
    
    Returns:
        dict: æ ¼å¼åŒ–åçš„æ ·æœ¬å­—å…¸
    """
    # æå–SMILESåˆ—å’Œå…¶ä»–ç‰¹å¾åˆ—
    smiles_cols = config['data']['single_file_config']['smiles_col']
    target_col = config['data']['single_file_config']['target_col']
    
    formatted_sample = {}
    
    # å¤åˆ¶SMILESåˆ—
    for col in smiles_cols:
        if col in sample_dict:
            formatted_sample[col] = sample_dict[col]
    
    # å¤åˆ¶å…¶ä»–éç›®æ ‡åˆ—
    precomputed_features = config['data']['single_file_config'].get('precomputed_features', {})
    feature_columns = precomputed_features.get('feature_columns')
    
    if feature_columns:
        # å¤„ç†é¢„è®¡ç®—ç‰¹å¾åˆ—
        for key, value in sample_dict.items():
            if key not in smiles_cols and key != target_col:
                formatted_sample[key] = value
    
    return formatted_sample

def create_validation_dataset(experiment_dir: str,
                            validation_size: int = 50,
                            use_test_if_no_val: bool = True,
                            split_aware: bool = False) -> Tuple[List[Dict], List]:
    """
    åˆ›å»ºç”¨äºæ¨¡å‹å †å éªŒè¯çš„æ•°æ®é›†
    
    Args:
        experiment_dir: å®éªŒç›®å½•è·¯å¾„
        validation_size: éªŒè¯æ•°æ®å¤§å°é™åˆ¶
        use_test_if_no_val: å¦‚æœæ²¡æœ‰éªŒè¯é›†ï¼Œæ˜¯å¦ä½¿ç”¨æµ‹è¯•é›†ï¼ˆå½“split_aware=Falseæ—¶ç”Ÿæ•ˆï¼‰
        split_aware: æ˜¯å¦æ ¹æ®åŸå®éªŒçš„split_modeæ™ºèƒ½é€‰æ‹©æ•°æ®é›†
                    True: train_valid_testæ¨¡å¼ä½¿ç”¨validationï¼Œcross_validationæ¨¡å¼ä½¿ç”¨test
                    False: æŒ‰ç…§use_test_if_no_valå‚æ•°çš„ä¼ ç»Ÿé€»è¾‘
    
    Returns:
        tuple: (éªŒè¯æ•°æ®åˆ—è¡¨, çœŸå®æ ‡ç­¾åˆ—è¡¨)
    """
    data_info = prepare_stacking_data(experiment_dir, use_original=True)
    config = data_info['config']
    
    # è·å–åŸå®éªŒçš„æ•°æ®æ‹†åˆ†æ¨¡å¼
    split_mode = config.get('split_mode', 'unknown')
    
    val_data = None
    val_labels = None
    source = None
    
    if split_aware:
        # æ ¹æ®åŸå®éªŒçš„split_modeæ™ºèƒ½é€‰æ‹©æ•°æ®é›†
        if split_mode == 'train_valid_test':
            # train_valid_testæ¨¡å¼ï¼šä¼˜å…ˆä½¿ç”¨validation setè¿›è¡Œå †å éªŒè¯
            if 'validation_data' in data_info:
                val_data = data_info['validation_data']
                val_labels = data_info['validation_labels']
                source = "validation"
                print(f"ğŸ¯ [split-aware] train_valid_testæ¨¡å¼ï¼šä½¿ç”¨validation setè¿›è¡Œå †å éªŒè¯")
            elif 'test_data' in data_info:
                val_data = data_info['test_data']
                val_labels = data_info['test_labels']
                source = "test"
                print(f"âš ï¸  [split-aware] æ²¡æœ‰validation setï¼Œä½¿ç”¨test setè¿›è¡Œå †å éªŒè¯")
            else:
                raise ValueError("train_valid_testæ¨¡å¼ä¸‹æ²¡æœ‰å¯ç”¨çš„validationæˆ–testæ•°æ®")
        
        elif split_mode == 'cross_validation':
            # cross_validationæ¨¡å¼ï¼šä½¿ç”¨test setè¿›è¡Œå †å éªŒè¯ï¼ˆå› ä¸ºæ²¡æœ‰ä¸“é—¨çš„validation setï¼‰
            if 'test_data' in data_info:
                val_data = data_info['test_data']
                val_labels = data_info['test_labels']
                source = "test"
                print(f"ğŸ¯ [split-aware] cross_validationæ¨¡å¼ï¼šä½¿ç”¨test setè¿›è¡Œå †å éªŒè¯")
            else:
                # å¦‚æœæ²¡æœ‰test setï¼Œä½¿ç”¨éƒ¨åˆ†trainæ•°æ®
                if 'validation_data' in data_info:
                    val_data = data_info['validation_data']
                    val_labels = data_info['validation_labels']
                    source = "validation(from_train)"
                    print(f"âš ï¸  [split-aware] cross_validationæ¨¡å¼ä¸‹æ²¡æœ‰test setï¼Œä½¿ç”¨è®­ç»ƒæ•°æ®çš„ä¸€éƒ¨åˆ†")
                else:
                    raise ValueError("cross_validationæ¨¡å¼ä¸‹æ²¡æœ‰å¯ç”¨çš„testæˆ–validationæ•°æ®")
        
        else:
            print(f"âš ï¸  [split-aware] æœªçŸ¥çš„split_mode: {split_mode}ï¼Œå›é€€åˆ°ä¼ ç»Ÿé€»è¾‘")
            split_aware = False  # å›é€€åˆ°ä¼ ç»Ÿé€»è¾‘
    
    if not split_aware:
        # ä¼ ç»Ÿé€»è¾‘ï¼šä¼˜å…ˆä½¿ç”¨éªŒè¯æ•°æ®
        if 'validation_data' in data_info:
            val_data = data_info['validation_data']
            val_labels = data_info['validation_labels']
            source = "validation"
        elif use_test_if_no_val and 'test_data' in data_info:
            val_data = data_info['test_data']
            val_labels = data_info['test_labels']
            source = "test"
            print("âš ï¸  æ²¡æœ‰éªŒè¯é›†ï¼Œä½¿ç”¨æµ‹è¯•é›†è¿›è¡Œå †å éªŒè¯")
        else:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„éªŒè¯æ•°æ®")
    
    if val_data is None or val_labels is None:
        raise ValueError("æ— æ³•è·å–éªŒè¯æ•°æ®")
    
    # é™åˆ¶æ•°æ®å¤§å°ä»¥æé«˜æ•ˆç‡
    if len(val_data) > validation_size:
        indices = np.random.choice(len(val_data), validation_size, replace=False)
        val_data = [val_data[i] for i in indices]
        val_labels = [val_labels[i] for i in indices]
        print(f"ğŸ¯ éšæœºé€‰æ‹© {validation_size} ä¸ªæ ·æœ¬è¿›è¡ŒéªŒè¯ï¼ˆä»{source}é›†ï¼‰")
    else:
        print(f"ğŸ¯ ä½¿ç”¨å…¨éƒ¨ {len(val_data)} ä¸ªæ ·æœ¬è¿›è¡ŒéªŒè¯ï¼ˆä»{source}é›†ï¼‰")
    
    # æ ¼å¼åŒ–æ ·æœ¬
    formatted_data = []
    for sample in val_data:
        formatted_sample = format_sample_for_prediction(sample, config)
        formatted_data.append(formatted_sample)
    
    return formatted_data, val_labels

def load_custom_validation_data(validation_file: str,
                               labels_file: Optional[str] = None,
                               target_column: Optional[str] = None) -> Tuple[List[Dict], List]:
    """
    åŠ è½½ç”¨æˆ·è‡ªå®šä¹‰çš„éªŒè¯æ•°æ®
    
    Args:
        validation_file: éªŒè¯æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆCSVï¼‰
        labels_file: æ ‡ç­¾æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œå¦‚æœNoneåˆ™ä»validation_fileä¸­è¯»å–ï¼‰
        target_column: ç›®æ ‡åˆ—åï¼ˆå¦‚æœlabels_fileä¸ºNoneæ—¶éœ€è¦ï¼‰
    
    Returns:
        tuple: (éªŒè¯æ•°æ®åˆ—è¡¨, çœŸå®æ ‡ç­¾åˆ—è¡¨)
    """
    # åŠ è½½éªŒè¯æ•°æ®
    val_df = pd.read_csv(validation_file)
    
    if labels_file is not None:
        # ä»å•ç‹¬çš„æ ‡ç­¾æ–‡ä»¶åŠ è½½
        labels_df = pd.read_csv(labels_file)
        if len(labels_df.columns) == 1:
            labels = labels_df.iloc[:, 0].tolist()
        else:
            raise ValueError("æ ‡ç­¾æ–‡ä»¶åº”è¯¥åªåŒ…å«ä¸€åˆ—")
        
        # éªŒè¯æ•°æ®ä¸åŒ…å«ç›®æ ‡åˆ—
        validation_data = val_df.to_dict('records')
    
    else:
        # ä»éªŒè¯æ–‡ä»¶ä¸­åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
        if target_column is None:
            raise ValueError("å¦‚æœä¸æä¾›å•ç‹¬çš„æ ‡ç­¾æ–‡ä»¶ï¼Œå¿…é¡»æŒ‡å®štarget_column")
        
        if target_column not in val_df.columns:
            raise ValueError(f"ç›®æ ‡åˆ— '{target_column}' ä¸åœ¨æ•°æ®ä¸­")
        
        labels = val_df[target_column].tolist()
        feature_df = val_df.drop(columns=[target_column])
        validation_data = feature_df.to_dict('records')
    
    print(f"âœ“ åŠ è½½è‡ªå®šä¹‰éªŒè¯æ•°æ®: {len(validation_data)} æ ·æœ¬")
    return validation_data, labels

# ç¤ºä¾‹ç”¨æ³•
if __name__ == "__main__":
    print("INTERNCRANE æ•°æ®åŠ è½½æ¨¡å—ç¤ºä¾‹")
    print("=" * 50)
    
    # ç¤ºä¾‹å®éªŒç›®å½•
    example_dir = "output/S04_agent_5_a_regression_20250101_120000"
    
    print("1. åŠ è½½å®éªŒé…ç½®:")
    try:
        config = load_experiment_config(example_dir)
        print(f"  å®éªŒåç§°: {config['experiment_name']}")
        print(f"  ä»»åŠ¡ç±»å‹: {config['task_type']}")
        print(f"  æ•°æ®æ‹†åˆ†æ¨¡å¼: {config['split_mode']}")
    except Exception as e:
        print(f"  âŒ é”™è¯¯: {e}")
    
    print("\n2. åŠ è½½åŸå§‹æ•°æ®æ‹†åˆ†:")
    try:
        data_splits = load_original_data_splits(example_dir)
        print(f"  å¯ç”¨æ•°æ®é›†: {list(data_splits.keys())}")
    except Exception as e:
        print(f"  âŒ é”™è¯¯: {e}")
    
    print("\n3. å‡†å¤‡å †å éªŒè¯æ•°æ®:")
    try:
        val_data, val_labels = create_validation_dataset(example_dir, validation_size=20)
        print(f"  éªŒè¯æ•°æ®: {len(val_data)} æ ·æœ¬")
        print(f"  ç¤ºä¾‹æ ·æœ¬é”®: {list(val_data[0].keys()) if val_data else []}")
        print(f"  æ ‡ç­¾èŒƒå›´: {min(val_labels):.3f} - {max(val_labels):.3f}")
    except Exception as e:
        print(f"  âŒ é”™è¯¯: {e}")
    
    print("\n4. è‡ªå®šä¹‰æ•°æ®ç¤ºä¾‹:")
    print("""
# ä½¿ç”¨è‡ªå®šä¹‰éªŒè¯æ•°æ®
val_data, val_labels = load_custom_validation_data(
    validation_file="my_validation_data.csv",
    target_column="target_value"
)

# æˆ–è€…ä½¿ç”¨åˆ†ç¦»çš„æ–‡ä»¶
val_data, val_labels = load_custom_validation_data(
    validation_file="my_features.csv",
    labels_file="my_labels.csv"
)
    """) 