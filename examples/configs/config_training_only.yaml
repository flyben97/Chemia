# ===================================================================
#      CRAFT - Training-Only Configuration v1.0
# ===================================================================
# This file is designed specifically for model training phase only.
# It includes comprehensive algorithm selection and hyperparameter optimization
# for both traditional ML and advanced algorithms suitable for small datasets.
# ===================================================================

experiment_name: "S04_agent_5_a"
task_type: "regression"  # 'regression' or 'classification'

# --- Data Configuration ---
data:
  source_mode: "single_file"
  single_file_config:
    main_file_path: "data/agent5_IB.csv"
    smiles_col: ["SMILES", "Solvent_1_SMILES", "Solvent_2_SMILES"]
    target_col: "Right_Time"  # The target column to predict
    precomputed_features:
      feature_columns: "3:11"  # Include Temp column as feature (from column 5)

# --- Training Configuration ---
training:
  # Comprehensive model selection - grouped by algorithm type
  models_to_run:
    # Gradient Boosting Methods (Best for most ML tasks)
    # - "ann"           # XGBoost
    # - "lgbm"          # LightGBM  
    # - "catboost"      # CatBoost
    # - "gbdt"          # Scikit-learn Gradient Boosting (NEW)
    - "xgb"  # Histogram-based GB
    
    # # Tree-based Ensemble Methods
    # - "rf"            # Random Forest
    # - "extratrees"    # Extra Trees (NEW)
    # - "adaboost"      # AdaBoost
    
    # # Linear Methods with Regularization (Good for small datasets)
    # - "ridge"         # Ridge Regression
    # - "elasticnet"    # ElasticNet (NEW)
    # - "lasso"         # LASSO (NEW)
    # - "bayesianridge" # Bayesian Ridge (NEW)
    
    # # Advanced Methods (Excellent for small datasets)
    # - "gpr"           # Gaussian Process Regression (NEW)
    # - "krr"           # Kernel Ridge Regression
    # - "svr"           # Support Vector Regression
    
    # # Neural Networks
    # - "ann"           # PyTorch-based Artificial Neural Network
    
    # # Simple but Effective Methods
    # - "kneighbors"    # k-Nearest Neighbors
    # - "sgd"           # Stochastic Gradient Descent (NEW)
  
  # Hyperparameter optimization settings
  n_trials: 30              # Increased for better optimization
  quiet_optuna: true        # Suppress Optuna output
  
  # Optuna optimization strategy
  optuna_config:
    sampler: "TPE"          # Tree-structured Parzen Estimator
    pruner: "MedianPruner"  # Early stopping for unpromising trials
    n_startup_trials: 10    # Random exploration before TPE

# --- Feature Engineering ---
features:
  per_smiles_col_generators:
    # Reduced complexity for small dataset to avoid overfitting
    SMILES: 
      # - type: "unimol"
      #   model_version: "v2"
      #   model_size: "310m"
      - type: "maccs"
      # - type: "morgan"
      #   nbits: 2048
      #   radius: 2
      # - type: "rdkit_descriptors"
    Solvent_1_SMILES: 
      - type: "rdkit_descriptors" 
      # - type: "maccs"
    Solvent_2_SMILES: 
      - type: "rdkit_descriptors"
      # - type: "maccs"  # Keep descriptors for ligand (most important)
  
  scaling: true             # Essential for many algorithms
  
  # Feature selection (for future implementation)
  # feature_selection:
  #   method: "variance_threshold"
  #   threshold: 0.01

split_mode: "train_valid_test"
split_config:
  train_valid_test:
    valid_size: 0.05
    test_size: 0.05
    random_state: 0

#  --- Cross-Validation Strategy ---
# split_mode: "cross_validation"
# split_config:
#   cross_validation:
#     n_splits: 5            # 5-fold cross-validation
#     test_size_for_cv: 0.05  # Keep 80% for training (reasonable test size)
#     random_state: 42
#     shuffle: true

# --- Model-specific Settings ---
# model_specific_configs:
#   # Small dataset optimizations
#   gpr:
#     optimize_restarts: 5    # Multiple restarts for GP optimization
   
#   xgb:
#     early_stopping_rounds: 20
#     eval_metric: "rmse"
  
#   lgbm:
#     early_stopping_rounds: 20
#     metric: "rmse"
  
#   catboost:
#     early_stopping_rounds: 20
#     eval_metric: "RMSE"

# --- Evaluation Metrics ---
evaluation:
  primary_metric: "r2"          # Primary metric for model selection
  additional_metrics:
    - "rmse"
    - "mae"
    - "mape"                    # Mean Absolute Percentage Error
  
  # # Cross-validation reporting
  # cv_reporting:
  #   show_fold_details: true
  #   show_std: true
  #   confidence_interval: 0.95

# --- Output Settings ---
output:
  save_predictions: true
  save_feature_importance: true
  save_cv_predictions: true
  save_hyperparameters: true
  generate_model_comparison: true
  
  # Model artifacts to save
  save_artifacts:
    - "best_model"
    - "scaler"
    - "feature_names"
    - "training_history"

# --- Computational Settings ---
computational:
  n_jobs: -1                # Use all available cores
  memory_efficient: true    # For large feature matrices
  random_state: 42
  
  # # Early stopping for ensemble methods
  # early_stopping:
  #   enabled: true
  #   patience: 20
  #   min_delta: 0.001

# --- Advanced Options ---
advanced:
  # Ensemble post-processing
  enable_stacking: false    # Can be enabled for advanced ensemble
  
  # Feature importance analysis
  feature_analysis:
    permutation_importance: true
    shap_analysis: false    # Computationally expensive
  
  # Model interpretation
  model_interpretation:
    save_partial_dependence: false  # For key features
    save_learning_curves: true 