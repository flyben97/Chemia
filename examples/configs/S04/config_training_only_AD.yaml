# ==============================================================================
#                 CHEMIA - Training-Only Configuration v1.0.0
# ==============================================================================
# This file is designed specifically for model training phase only.
# It includes comprehensive algorithm selection and hyperparameter optimization
# for both traditional ML and advanced algorithms suitable for small datasets.
# ==============================================================================

experiment_name: "S04_agent_5_AD"
task_type: "regression"  # 'regression' or 'classification'

# --- Data Configuration ---
data:
  source_mode: "single_file"
  single_file_config:
    main_file_path: "data/agent_AD_with_smiles.csv"
    smiles_col: ["SMILES", "Solvent_1_SMILES", "Solvent_2_SMILES"]  # SMILES columns for features
    target_col: "RT2"  # The target column to predict
    precomputed_features:
      feature_columns: "3:11"  # Include Temp column as feature (from column 5)

# --- Training Configuration ---
training:
  # Comprehensive model selection - grouped by algorithm type
  models_to_run:
    # - "xgb"           # XGBoost
    # - "lgbm"          # LightGBM  
    # - "catboost"      # CatBoost
    - "gbdt"          # Gradient Boosting
    # - "hgb"           # Histogram-based GB    
    # - "rf"            # Random Forest
    - "extratrees"    # Extra Trees
    # - "adaboost"      # AdaBoost
    # - "ridge"         # Ridge Regression
    - "gpr"           # Gaussian Process Regression
    # - "krr"           # Kernel Ridge Regression
    # - "svr"           # Support Vector Regression
    # - "ann"           # PyTorch-based Artificial Neural Network
  
  # Hyperparameter optimization settings
  n_trials: 50              # Increased for better optimization
  quiet_optuna: true        # Suppress Optuna output
  
  # Optuna optimization strategy
  optuna_config:
    sampler: "TPE"          # Tree-structured Parzen Estimator
    pruner: "MedianPruner"  # Early stopping for unpromising trials
    n_startup_trials: 10    # Random exploration before TPE

# --- Feature Engineering ---
features:
  per_smiles_col_generators:
    # Reduced complexity for small dataset to avoid overfitting
    SMILES: 
      # - type: "unimol"
      #   model_version: "v2"
      #   model_size: "310m"
      - type: "maccs"
      - type: "morgan"
        nbits: 2048
        radius: 2
      # - type: "rdkit_descriptors"
    Solvent_1_SMILES: 
      - type: "rdkit_descriptors" 
    Solvent_2_SMILES: 
      - type: "rdkit_descriptors"  
  
  scaling: true             # Essential for many algorithms

split_mode: "train_valid_test"
split_config:
  train_valid_test:
    valid_size: 0.05
    test_size: 0.1
    random_state: 42

# --- Evaluation Metrics ---
evaluation:
  primary_metric: "r2"          # Primary metric for model selection
  additional_metrics:
    - "rmse"
    - "mae"
    - "mape"                    # Mean Absolute Percentage Error

# --- Output Settings ---
output:
  save_predictions: true
  save_feature_importance: true
  save_cv_predictions: true
  save_hyperparameters: true
  generate_model_comparison: true
  
  # Model artifacts to save
  save_artifacts:
    - "best_model"
    - "scaler"
    - "feature_names"
    - "training_history"

# --- Computational Settings ---
computational:
  n_jobs: 6                # Use all available cores
  memory_efficient: true    # For large feature matrices
  random_state: 42

# --- Advanced Options ---
advanced:
  # Ensemble post-processing
  enable_stacking: false    # Can be enabled for advanced ensemble
  
  # Feature importance analysis
  feature_analysis:
    permutation_importance: true
    shap_analysis: false    # Computationally expensive
  
  # Model interpretation
  model_interpretation:
    save_partial_dependence: false  # For key features
    save_learning_curves: true 