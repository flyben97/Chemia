#!/usr/bin/env python3
"""
CRAFT å †å é›†æˆå·¥å…·æ¨¡å—

è¿™ä¸ªæ¨¡å—æä¾›å„ç§ä¾¿æ·çš„å †å é›†æˆåˆ›å»ºå‡½æ•°ï¼ŒåŒ…æ‹¬ï¼š
- å¿«é€Ÿåˆ›å»ºé›†æˆæ¨¡å‹
- è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æ¨¡å‹ç»„åˆ  
- æ™ºèƒ½å…ƒå­¦ä¹ å™¨é›†æˆ
- æ¨¡å‹æ€§èƒ½è¯„ä¼°å’Œæ¯”è¾ƒ
"""

import os
import sys
import contextlib
import numpy as np
from typing import Dict, List, Any, Union, Optional
from sklearn.metrics import r2_score, accuracy_score

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from prediction_api import load_model
from data_loader import create_validation_dataset

def create_ensemble(experiment_dir: str, model_names: List[str], 
                   weights: Optional[List[float]] = None,
                   method: str = "weighted_average"):
    """
    å¿«é€Ÿåˆ›å»ºæ¨¡å‹é›†æˆ
    
    Args:
        experiment_dir: å®éªŒç›®å½•è·¯å¾„
        model_names: æ¨¡å‹åç§°åˆ—è¡¨
        weights: æ¨¡å‹æƒé‡åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
        method: å †å æ–¹æ³•
    
    Returns:
        ModelStacker: é…ç½®å¥½çš„å †å å™¨
    """
    from model_stacking import ModelStacker
    
    if weights is None:
        weights = [1.0] * len(model_names)
    
    if len(weights) != len(model_names):
        raise ValueError("æƒé‡æ•°é‡å¿…é¡»ä¸æ¨¡å‹æ•°é‡ä¸€è‡´")
    
    # åˆ›å»ºå †å å™¨
    stacker = ModelStacker(experiment_dir=experiment_dir)
    stacker.set_stacking_method(method)
    
    # æ·»åŠ æ¨¡å‹
    for model_name, weight in zip(model_names, weights):
        stacker.add_model(model_name, weight)
    
    return stacker

def auto_ensemble(experiment_dir: str, 
                 validation_data: Optional[Union[Dict, List[Dict]]] = None,
                 true_labels: Optional[Union[List, np.ndarray]] = None,
                 available_models: Optional[List[str]] = None,
                 auto_load_validation: bool = True,
                 validation_size: int = 100,
                 split_aware: bool = False):
    """
    è‡ªåŠ¨åˆ›å»ºæœ€ä¼˜é›†æˆæ¨¡å‹
    
    Args:
        experiment_dir: å®éªŒç›®å½•è·¯å¾„
        validation_data: éªŒè¯æ•°æ®ï¼ˆå¯é€‰ï¼‰
        true_labels: çœŸå®æ ‡ç­¾ï¼ˆå¯é€‰ï¼‰
        available_models: å¯ç”¨æ¨¡å‹åˆ—è¡¨
        auto_load_validation: æ˜¯å¦è‡ªåŠ¨åŠ è½½éªŒè¯æ•°æ®
        validation_size: éªŒè¯æ•°æ®å¤§å°é™åˆ¶
        split_aware: æ˜¯å¦æ ¹æ®åŸå®éªŒçš„split_modeæ™ºèƒ½é€‰æ‹©æ•°æ®é›†
    
    Returns:
        ModelStacker: æœ€ä¼˜å †å å™¨
    """
    from model_stacking import ModelStacker
    
    if available_models is None:
        available_models = ['xgb', 'lgbm', 'catboost', 'rf', 'ann']
    
    print("ğŸ” è‡ªåŠ¨å¯»æ‰¾æœ€ä¼˜é›†æˆæ¨¡å‹...")
    
    # è‡ªåŠ¨åŠ è½½éªŒè¯æ•°æ®
    if validation_data is None and true_labels is None and auto_load_validation:
        try:
            print("ğŸ”„ è‡ªåŠ¨ä»å®éªŒç›®å½•åŠ è½½éªŒè¯æ•°æ®...")
            validation_data, true_labels = create_validation_dataset(
                experiment_dir, 
                validation_size=validation_size,
                split_aware=split_aware
            )
            print(f"âœ“ è‡ªåŠ¨åŠ è½½éªŒè¯æ•°æ®: {len(validation_data)} æ ·æœ¬")
        except Exception as e:
            raise ValueError(f"è‡ªåŠ¨åŠ è½½éªŒè¯æ•°æ®å¤±è´¥: {e}. è¯·æ‰‹åŠ¨æä¾›validation_dataå’Œtrue_labels")
    
    if validation_data is None or true_labels is None:
        raise ValueError("å¿…é¡»æä¾›validation_dataå’Œtrue_labelsï¼Œæˆ–è®¾ç½®auto_load_validation=True")
    
    # åŠ è½½å¹¶è¯„ä¼°å„ä¸ªæ¨¡å‹
    valid_models = []
    model_scores = {}
    
    for model_name in available_models:
        try:
            with contextlib.suppress(Exception):
                predictor = load_model(experiment_dir, model_name)
                result = predictor.predict(validation_data)
                predictions = result['predictions']
                
                if predictor.task_type == 'regression':
                    score = r2_score(true_labels, predictions)
                else:
                    score = accuracy_score(true_labels, predictions)
                
                valid_models.append(model_name)
                model_scores[model_name] = score
                print(f"  âœ“ {model_name}: {score:.4f}")
        except Exception:
            print(f"  âŒ {model_name}: æ— æ³•åŠ è½½")
    
    if len(valid_models) < 2:
        raise ValueError("è‡³å°‘éœ€è¦2ä¸ªæœ‰æ•ˆæ¨¡å‹æ‰èƒ½åˆ›å»ºé›†æˆ")
    
    # æ ¹æ®æ€§èƒ½è®¡ç®—æƒé‡
    scores = np.array([model_scores[name] for name in valid_models])
    scores = np.maximum(scores, 0)  # ç¡®ä¿éè´Ÿ
    weights = scores / np.sum(scores) if np.sum(scores) > 0 else np.ones(len(scores)) / len(scores)
    
    # åˆ›å»ºåŠ æƒé›†æˆ
    stacker = create_ensemble(experiment_dir, valid_models, weights.tolist(), "weighted_average")
    
    print(f"âœ“ åˆ›å»ºé›†æˆæ¨¡å‹ï¼ŒåŒ…å« {len(valid_models)} ä¸ªæ¨¡å‹")
    for name, weight in zip(valid_models, weights):
        print(f"  - {name}: {weight:.3f}")
    
    return stacker

def smart_ensemble_with_meta_learner(experiment_dir: str,
                                   validation_data: Optional[Union[Dict, List[Dict]]] = None,
                                   true_labels: Optional[Union[List, np.ndarray]] = None,
                                   available_models: Optional[List[str]] = None,
                                   auto_load_validation: bool = True,
                                   validation_size: int = 100,
                                   meta_learner: str = "ridge",
                                   split_aware: bool = False):
    """
    åˆ›å»ºå¸¦æœ‰å…ƒå­¦ä¹ å™¨çš„æ™ºèƒ½é›†æˆæ¨¡å‹
    
    Args:
        experiment_dir: å®éªŒç›®å½•è·¯å¾„
        validation_data: éªŒè¯æ•°æ®ï¼ˆå¯é€‰ï¼‰
        true_labels: çœŸå®æ ‡ç­¾ï¼ˆå¯é€‰ï¼‰
        available_models: å¯ç”¨æ¨¡å‹åˆ—è¡¨
        auto_load_validation: æ˜¯å¦è‡ªåŠ¨åŠ è½½éªŒè¯æ•°æ®
        validation_size: éªŒè¯æ•°æ®å¤§å°é™åˆ¶
        meta_learner: å…ƒå­¦ä¹ å™¨ç±»å‹ ("ridge", "rf", "logistic")
        split_aware: æ˜¯å¦æ ¹æ®åŸå®éªŒçš„split_modeæ™ºèƒ½é€‰æ‹©æ•°æ®é›†
    
    Returns:
        ModelStacker: æ™ºèƒ½å †å å™¨
    """
    from model_stacking import ModelStacker
    
    if available_models is None:
        available_models = ['xgb', 'lgbm', 'catboost', 'rf', 'ann']
    
    print("ğŸ§  åˆ›å»ºæ™ºèƒ½å…ƒå­¦ä¹ å™¨é›†æˆ...")
    
    # è‡ªåŠ¨åŠ è½½éªŒè¯æ•°æ®
    if validation_data is None and true_labels is None and auto_load_validation:
        try:
            print("ğŸ”„ è‡ªåŠ¨ä»å®éªŒç›®å½•åŠ è½½éªŒè¯æ•°æ®...")
            validation_data, true_labels = create_validation_dataset(
                experiment_dir, 
                validation_size=validation_size,
                split_aware=split_aware
            )
            print(f"âœ“ è‡ªåŠ¨åŠ è½½éªŒè¯æ•°æ®: {len(validation_data)} æ ·æœ¬")
        except Exception as e:
            raise ValueError(f"è‡ªåŠ¨åŠ è½½éªŒè¯æ•°æ®å¤±è´¥: {e}. è¯·æ‰‹åŠ¨æä¾›validation_dataå’Œtrue_labels")
    
    if validation_data is None or true_labels is None:
        raise ValueError("å¿…é¡»æä¾›validation_dataå’Œtrue_labelsï¼Œæˆ–è®¾ç½®auto_load_validation=True")
    
    # åŠ è½½æ‰€æœ‰å¯ç”¨æ¨¡å‹
    valid_models = []
    for model_name in available_models:
        try:
            with contextlib.suppress(Exception):
                predictor = load_model(experiment_dir, model_name)
                valid_models.append(model_name)
                print(f"  âœ“ {model_name}: åŠ è½½æˆåŠŸ")
        except Exception:
            print(f"  âŒ {model_name}: æ— æ³•åŠ è½½")
    
    if len(valid_models) < 2:
        raise ValueError("è‡³å°‘éœ€è¦2ä¸ªæœ‰æ•ˆæ¨¡å‹æ‰èƒ½åˆ›å»ºé›†æˆ")
    
    # åˆ›å»ºå †å å™¨å¹¶è®¾ç½®å…ƒå­¦ä¹ å™¨
    stacker = ModelStacker(experiment_dir=experiment_dir)
    stacker.set_stacking_method(meta_learner)
    
    # æ·»åŠ æ‰€æœ‰æœ‰æ•ˆæ¨¡å‹
    for model_name in valid_models:
        stacker.add_model(model_name, weight=1.0)
    
    # è®­ç»ƒå…ƒå­¦ä¹ å™¨
    stacker.fit_meta_model(
        validation_data=validation_data,
        true_labels=true_labels,
        auto_load=False  # å·²ç»æ‰‹åŠ¨æä¾›äº†æ•°æ®
    )
    
    print(f"âœ“ æ™ºèƒ½é›†æˆæ¨¡å‹åˆ›å»ºå®Œæˆï¼ŒåŒ…å« {len(valid_models)} ä¸ªæ¨¡å‹")
    print(f"âœ“ å…ƒå­¦ä¹ å™¨: {meta_learner}")
    
    return stacker

def compare_ensemble_methods(experiment_dir: str,
                           model_names: Optional[List[str]] = None,
                           methods: Optional[List[str]] = None,
                           validation_size: int = 100) -> Dict[str, Dict[str, Any]]:
    """
    æ¯”è¾ƒä¸åŒå †å æ–¹æ³•çš„æ€§èƒ½
    
    Args:
        experiment_dir: å®éªŒç›®å½•è·¯å¾„
        model_names: æ¨¡å‹åç§°åˆ—è¡¨
        methods: å †å æ–¹æ³•åˆ—è¡¨
        validation_size: éªŒè¯æ•°æ®å¤§å°
    
    Returns:
        dict: å„æ–¹æ³•çš„è¯„ä¼°ç»“æœ
    """
    if model_names is None:
        model_names = ['xgb', 'lgbm', 'catboost']
    
    if methods is None:
        methods = ['simple_average', 'weighted_average', 'ridge']
    
    results = {}
    
    print("ğŸ”¬ æ¯”è¾ƒä¸åŒå †å æ–¹æ³•...")
    
    for method in methods:
        try:
            print(f"ğŸ”„ æµ‹è¯•æ–¹æ³•: {method}")
            
            if method in ['simple_average', 'weighted_average']:
                stacker = create_ensemble(experiment_dir, model_names, method=method)
            else:
                stacker = smart_ensemble_with_meta_learner(
                    experiment_dir=experiment_dir,
                    available_models=model_names,
                    validation_size=validation_size,
                    meta_learner=method
                )
            
            evaluation = stacker.evaluate(auto_load=True)
            results[method] = evaluation
            
            # æ˜¾ç¤ºä¸»è¦æŒ‡æ ‡
            if stacker.task_type == 'regression':
                r2 = evaluation.get('r2', 'N/A')
                rmse = evaluation.get('rmse', 'N/A')
                print(f"  âœ“ RÂ²: {r2:.4f}, RMSE: {rmse:.4f}")
            else:
                acc = evaluation.get('accuracy', 'N/A')
                print(f"  âœ“ Accuracy: {acc:.4f}")
                
        except Exception as e:
            print(f"  âŒ å¤±è´¥: {e}")
            results[method] = {'error': str(e)}
    
    return results

def find_available_models(experiment_dir: str) -> List[str]:
    """
    æŸ¥æ‰¾å®éªŒç›®å½•ä¸­å¯ç”¨çš„æ¨¡å‹
    
    Args:
        experiment_dir: å®éªŒç›®å½•è·¯å¾„
    
    Returns:
        list: å¯ç”¨æ¨¡å‹åç§°åˆ—è¡¨
    """
    available_models = []
    candidate_models = ['xgb', 'lgbm', 'catboost', 'rf', 'ann', 'gbdt', 'extratrees']
    
    for model_name in candidate_models:
        try:
            # å°è¯•åŠ è½½æ¨¡å‹æ¥æ£€æŸ¥æ˜¯å¦å­˜åœ¨
            load_model(experiment_dir, model_name)
            available_models.append(model_name)
        except Exception:
            pass
    
    return available_models

def get_ensemble_recommendations(experiment_dir: str) -> Dict[str, Any]:
    """
    è·å–é›†æˆæ–¹æ³•æ¨è
    
    Args:
        experiment_dir: å®éªŒç›®å½•è·¯å¾„
    
    Returns:
        dict: æ¨èä¿¡æ¯
    """
    available_models = find_available_models(experiment_dir)
    n_models = len(available_models)
    
    recommendations = {
        'available_models': available_models,
        'n_models': n_models,
        'recommendations': []
    }
    
    if n_models < 2:
        recommendations['recommendations'].append({
            'type': 'warning',
            'message': f"åªæ‰¾åˆ° {n_models} ä¸ªæ¨¡å‹ï¼Œè‡³å°‘éœ€è¦2ä¸ªæ¨¡å‹æ‰èƒ½è¿›è¡Œé›†æˆ"
        })
    elif n_models == 2:
        recommendations['recommendations'].extend([
            {
                'type': 'method',
                'name': 'simple_average',
                'reason': 'æ¨¡å‹æ•°é‡è¾ƒå°‘ï¼Œç®€å•å¹³å‡å³å¯'
            },
            {
                'type': 'method', 
                'name': 'weighted_average',
                'reason': 'å¯ä»¥æ ¹æ®æ€§èƒ½æ‰‹åŠ¨è®¾ç½®æƒé‡'
            }
        ])
    else:
        recommendations['recommendations'].extend([
            {
                'type': 'method',
                'name': 'auto_ensemble',
                'reason': 'è‡ªåŠ¨é€‰æ‹©æœ€ä½³æƒé‡ç»„åˆ'
            },
            {
                'type': 'method',
                'name': 'smart_ensemble_with_meta_learner',
                'reason': 'ä½¿ç”¨å…ƒå­¦ä¹ å™¨è‡ªåŠ¨å­¦ä¹ æœ€ä¼˜ç»„åˆç­–ç•¥'
            }
        ])
    
    return recommendations 