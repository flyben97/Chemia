#!/usr/bin/env python3
"""
CRAFT å †å è¯„ä¼°å·¥å…·æ¨¡å—

è¿™ä¸ªæ¨¡å—æä¾›å †å æ¨¡åž‹çš„è¯„ä¼°ã€åˆ†æžå’Œå¯è§†åŒ–åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- æ¨¡åž‹æ€§èƒ½è¯„ä¼°
- åŸºç¡€æ¨¡åž‹å¯¹æ¯”åˆ†æž
- å †å æ•ˆæžœåˆ†æž
- è¯„ä¼°ç»“æžœå¯¼å‡º
"""

import os
import sys
import numpy as np
import pandas as pd
from typing import Dict, List, Any, Union, Optional
from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, log_loss

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from data_loader import prepare_stacking_data

def evaluate_stacking_performance(stacker, 
                                test_data: Optional[Union[Dict, List[Dict], pd.DataFrame]] = None,
                                true_labels: Optional[Union[List, np.ndarray]] = None,
                                auto_load: bool = True,
                                use_test_set: bool = True,
                                evaluate_both_sets: bool = True) -> Dict[str, Any]:
    """
    è¯„ä¼°å †å æ¨¡åž‹æ€§èƒ½
    
    Args:
        stacker: å †å æ¨¡åž‹å®žä¾‹
        test_data: æµ‹è¯•æ•°æ®ï¼ˆå¯é€‰ï¼‰
        true_labels: çœŸå®žæ ‡ç­¾ï¼ˆå¯é€‰ï¼‰
        auto_load: æ˜¯å¦è‡ªåŠ¨åŠ è½½æ•°æ®
        use_test_set: æ˜¯å¦ä½¿ç”¨æµ‹è¯•é›†ï¼ˆå½“evaluate_both_sets=Falseæ—¶ç”Ÿæ•ˆï¼‰
        evaluate_both_sets: æ˜¯å¦åŒæ—¶è¯„ä¼°validationå’Œtestæ•°æ®é›†
    
    Returns:
        dict: è¯„ä¼°ç»“æžœï¼ŒåŒ…å«å•ä¸ªæˆ–ä¸¤ä¸ªæ•°æ®é›†çš„ç»“æžœ
    """
    
    if not auto_load:
        # å¦‚æžœä¸è‡ªåŠ¨åŠ è½½ï¼Œä½¿ç”¨æä¾›çš„æ•°æ®è¿›è¡Œå•ä¸€è¯„ä¼°
        return _evaluate_single_dataset(stacker, test_data, true_labels, "provided_data")
    
    if stacker.experiment_dir is None:
        raise ValueError("éœ€è¦è®¾ç½®experiment_diræˆ–æä¾›test_dataå’Œtrue_labels")
    
    data_info = prepare_stacking_data(stacker.experiment_dir)
    
    if not evaluate_both_sets:
        # åŽŸæœ‰é€»è¾‘ï¼šåªè¯„ä¼°ä¸€ä¸ªæ•°æ®é›†
        if use_test_set and 'test_data' in data_info:
            test_data = data_info['test_data']
            true_labels = data_info['test_labels']
            if test_data is not None:
                print(f"âœ“ è‡ªåŠ¨åŠ è½½æµ‹è¯•æ•°æ®: {len(test_data)} æ ·æœ¬")
            return _evaluate_single_dataset(stacker, test_data, true_labels, "test")
        elif 'validation_data' in data_info:
            test_data = data_info['validation_data']
            true_labels = data_info['validation_labels']
            if test_data is not None:
                print(f"âœ“ è‡ªåŠ¨åŠ è½½éªŒè¯æ•°æ®ç”¨äºŽè¯„ä¼°: {len(test_data)} æ ·æœ¬")
            return _evaluate_single_dataset(stacker, test_data, true_labels, "validation")
        else:
            raise ValueError("æ— æ³•åŠ è½½æµ‹è¯•æˆ–éªŒè¯æ•°æ®")
    
    # æ–°åŠŸèƒ½ï¼šåŒæ—¶è¯„ä¼°validationå’Œtestæ•°æ®é›†
    evaluation_results = {
        'stacking_method': stacker.stacking_method,
        'n_models': len(stacker.base_models),
        'model_names': list(stacker.base_models.keys()),
        'evaluation_mode': 'both_sets'
    }
    
    datasets_evaluated = []
    
    # è¯„ä¼°validationæ•°æ®é›†
    if 'validation_data' in data_info and data_info['validation_data'] is not None:
        val_data = data_info['validation_data']
        val_labels = data_info['validation_labels']
        val_size = len(val_data) if val_data is not None else 0
        print(f"âœ“ è‡ªåŠ¨åŠ è½½éªŒè¯æ•°æ®: {val_size} æ ·æœ¬")
        
        val_evaluation = _evaluate_single_dataset(stacker, val_data, val_labels, "validation")
        evaluation_results['validation_set'] = val_evaluation
        datasets_evaluated.append('validation')
    
    # è¯„ä¼°testæ•°æ®é›†
    if 'test_data' in data_info and data_info['test_data'] is not None:
        test_data = data_info['test_data']
        test_labels = data_info['test_labels']
        test_size = len(test_data) if test_data is not None else 0
        print(f"âœ“ è‡ªåŠ¨åŠ è½½æµ‹è¯•æ•°æ®: {test_size} æ ·æœ¬")
        
        test_evaluation = _evaluate_single_dataset(stacker, test_data, test_labels, "test")
        evaluation_results['test_set'] = test_evaluation
        datasets_evaluated.append('test')
    
    if not datasets_evaluated:
        raise ValueError("æ— æ³•åŠ è½½ä»»ä½•è¯„ä¼°æ•°æ®é›†")
    
    evaluation_results['datasets_evaluated'] = datasets_evaluated
    
    return evaluation_results


def _evaluate_single_dataset(stacker, data, true_labels, dataset_name: str) -> Dict[str, Any]:
    """è¯„ä¼°å•ä¸ªæ•°æ®é›†çš„æ€§èƒ½"""
    if data is None or true_labels is None:
        raise ValueError(f"æ•°æ®é›† {dataset_name} çš„æ•°æ®æˆ–æ ‡ç­¾ä¸ºç©º")
    
    result = stacker.predict(data)
    predictions = result['predictions']
    y_true = np.array(true_labels)
    
    evaluation = {
        'dataset_name': dataset_name,
        'n_samples': len(y_true),
        'stacking_method': stacker.stacking_method,
        'model_names': list(stacker.base_models.keys())
    }
    
    # è®¡ç®—å †å æ¨¡åž‹æ€§èƒ½
    if stacker.task_type == 'regression':
        mse = mean_squared_error(y_true, predictions)
        r2 = r2_score(y_true, predictions)
        rmse = np.sqrt(mse)
        mae = np.mean(np.abs(y_true - predictions))
        
        evaluation.update({
            'mse': mse,
            'rmse': rmse,
            'mae': mae,
            'r2': r2
        })
    else:
        accuracy = accuracy_score(y_true, predictions)
        evaluation['accuracy'] = accuracy
        
        if result['probabilities'] is not None:
            try:
                logloss = log_loss(y_true, result['probabilities'])
                evaluation['log_loss'] = logloss
            except Exception:
                pass
    
    # è¯„ä¼°å„ä¸ªåŸºç¡€æ¨¡åž‹çš„æ€§èƒ½
    base_predictions = result['base_predictions']
    model_names = result['model_names']
    
    base_performance = {}
    for i, model_name in enumerate(model_names):
        base_pred = base_predictions[:, i]
        if stacker.task_type == 'regression':
            base_r2 = r2_score(y_true, base_pred)
            base_rmse = np.sqrt(mean_squared_error(y_true, base_pred))
            base_mae = np.mean(np.abs(y_true - base_pred))
            base_performance[model_name] = {'r2': base_r2, 'rmse': base_rmse, 'mae': base_mae}
        else:
            base_acc = accuracy_score(y_true, base_pred)
            base_performance[model_name] = {'accuracy': base_acc}
    
    evaluation['base_model_performance'] = base_performance
    
    return evaluation

def generate_evaluation_report(evaluation: Dict[str, Any]) -> str:
    """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
    report = []
    report.append("=" * 60)
    report.append("CRAFT æ¨¡åž‹å †å è¯„ä¼°æŠ¥å‘Š")
    report.append("=" * 60)
    
    # åŸºæœ¬ä¿¡æ¯
    report.append(f"å †å æ–¹æ³•: {evaluation['stacking_method']}")
    report.append(f"æ¨¡åž‹æ•°é‡: {evaluation['n_models']}")
    report.append(f"æ¨¡åž‹åˆ—è¡¨: {', '.join(evaluation['model_names'])}")
    report.append("")
    
    # å †å æ¨¡åž‹æ€§èƒ½
    report.append("ðŸ“Š å †å æ¨¡åž‹æ€§èƒ½:")
    report.append("-" * 30)
    
    if 'r2' in evaluation:
        report.append(f"RÂ² Score: {evaluation['r2']:.4f}")
        report.append(f"RMSE: {evaluation['rmse']:.4f}")
        report.append(f"MAE: {evaluation['mae']:.4f}")
        report.append(f"MSE: {evaluation['mse']:.4f}")
    elif 'accuracy' in evaluation:
        report.append(f"Accuracy: {evaluation['accuracy']:.4f}")
        if 'log_loss' in evaluation:
            report.append(f"Log Loss: {evaluation['log_loss']:.4f}")
    
    report.append("")
    
    # åŸºç¡€æ¨¡åž‹æ€§èƒ½
    report.append("ðŸ” åŸºç¡€æ¨¡åž‹æ€§èƒ½å¯¹æ¯”:")
    report.append("-" * 30)
    
    base_performance = evaluation.get('base_model_performance', {})
    for model_name, metrics in base_performance.items():
        if 'r2' in metrics:
            report.append(f"{model_name:>10}: RÂ²={metrics['r2']:.4f}, RMSE={metrics['rmse']:.4f}")
        elif 'accuracy' in metrics:
            report.append(f"{model_name:>10}: Accuracy={metrics['accuracy']:.4f}")
    
    return "\n".join(report)
